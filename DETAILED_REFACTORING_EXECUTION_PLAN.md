# Detailed Refactoring Execution Plan

**Date:** November 4, 2025
**Plan Type:** Comprehensive 4-Week Integrated Refactor
**Scope:** Database + API + Pattern System + UI
**Status:** ðŸ“‹ PLANNING COMPLETE - Ready for Execution

---

## ðŸŽ¯ Executive Summary

This detailed execution plan integrates findings from:
- [DATABASE_SCHEMA_ANALYSIS.md](DATABASE_SCHEMA_ANALYSIS.md) - Database layer issues
- [COMPREHENSIVE_SYSTEM_ANALYSIS_INTEGRATION.md](COMPREHENSIVE_SYSTEM_ANALYSIS_INTEGRATION.md) - Multi-layer integration
- [PHASE_3_CLEANUP_PLAN_V2.md](PHASE_3_CLEANUP_PLAN_V2.md) - Code cleanup patterns
- Replit Agent API Testing findings

### Critical Dependencies Confirmed

```
WEEK 0: Database Field Names (P0)
  â†“ BLOCKS
WEEK 1-2: Pattern System Refactoring
  â†“ DEPENDS ON
WEEK 3: Complete System Fixes
  â†“ VALIDATES
WEEK 4: End-to-End Testing
```

**Why These Dependencies Matter:**
1. **Pattern system refactoring CANNOT proceed without standardized field names** because:
   - Pattern JSON files reference fields by name (`quantity`, `market_value`)
   - UI `patternRegistry` has 46 `dataPath` mappings that expect specific field names
   - Field name mismatches cause pattern failures (already happening)

2. **Database integrity fixes MUST happen first** because:
   - Missing FK constraints cause orphaned records
   - Orphaned records cause pattern step failures
   - Cannot validate patterns with corrupt data

3. **suggest_hedges capability is already fixed** (line 2832 in financial_analyst.py):
   - Method exists: `financial_analyst_suggest_hedges()`
   - Only needs registration verification

---

## ðŸ“Š Comprehensive Codebase Analysis

### Field Name Usage Mapping

| Location | qty/qty_open | quantity | market_value | value | Total Lines |
|----------|-------------|----------|--------------|-------|-------------|
| **Database Schema** | 51 (qty_open) | 105 | 33 | 12 | 201 |
| **Backend Agents** | 89 | 67 | 45 | 18 | 219 |
| **Backend Services** | 51 | 23 | 38 | 15 | 127 |
| **Pattern JSON** | 0 | 5 | 12 | 8 | 25 |
| **UI (full_ui.html)** | 0 | 34 | 56 | 23 | 113 |
| **TOTAL** | **191** | **204** | **184** | **76** | **685** |

**Key Findings:**
- 191 locations use `qty` or `qty_open` (abbreviation)
- 204 locations use `quantity` (standard)
- 76 locations use ambiguous `value` instead of `market_value`
- **685 total locations** need review for standardization

### Pattern Registry and DataPath Analysis

**Location:** [full_ui.html:2832-3451](full_ui.html#L2832-L3451)

**Pattern Registry Structure:**
```javascript
const patternRegistry = {
    'portfolio_overview': {
        name: 'Portfolio Overview',
        panels: [
            {
                id: 'performance_strip',
                type: 'metrics',
                dataPath: 'perf_metrics',  // â† Maps to pattern output
                // ...
            },
            {
                id: 'holdings',
                type: 'table',
                dataPath: 'valued_positions.positions',  // â† Nested path
                // Expects: [{quantity, market_value, symbol, ...}]
            }
        ]
    },
    // ... 12 more patterns
}
```

**Total DataPath Mappings:** 46 across 13 patterns

**Critical Dependencies:**
1. `valued_positions.positions` (portfolio_overview) â†’ Expects `quantity`, `market_value`
2. `scenario_result.position_deltas` â†’ Expects `quantity`, `value` (inconsistent!)
3. `rebalance_result.trades` â†’ Expects `quantity`, `market_value`

**Impact:** If backend returns `qty_open`, UI dataPath fails to find data â†’ blank panels

### Database Schema Files

**Schema Files (11):**
```
backend/db/schema/
â”œâ”€â”€ 000_roles.sql
â”œâ”€â”€ 001_portfolios_lots_transactions.sql  â† qty_open defined here (line 72)
â”œâ”€â”€ 002_pricing_packs.sql
â”œâ”€â”€ 003_portfolio_metrics.sql
â”œâ”€â”€ 004_portfolio_daily_values.sql  â† valuation_date vs asof_date
â”œâ”€â”€ 005_portfolio_cash_flows.sql
â”œâ”€â”€ 006_macro_indicators.sql
â”œâ”€â”€ 007_alerts_notifications.sql
â”œâ”€â”€ 008_scenario_factor_tables.sql
â”œâ”€â”€ 009_audit_log.sql
â””â”€â”€ 010_corporate_actions.sql
```

**Migration Files (9):**
```
backend/db/migrations/
â”œâ”€â”€ 005_rls_policies.sql
â”œâ”€â”€ 007_add_lot_qty_tracking.sql  â† Adds qty_open, qty_original
â”œâ”€â”€ 008_add_corporate_actions_support.sql
â”œâ”€â”€ 009_add_scenario_dar_tables.sql  â† DUPLICATE TABLE DEFINITIONS
â”œâ”€â”€ 009_jwt_auth.sql  â† CONFLICTING NUMBER
â”œâ”€â”€ 010_add_users_and_audit_log.sql
â”œâ”€â”€ 010_fix_audit_log_schema.sql  â† CONFLICTING NUMBER
â”œâ”€â”€ 011_alert_delivery_system.sql
â””â”€â”€ 013_derived_indicators.sql
```

**Critical Issues Found:**
1. Migration 007 adds `qty_open`, `qty_original` but base table has `quantity` â†’ 3-way conflict
2. Migration 009 duplicated (scenario tables + JWT auth)
3. Migration 010 duplicated (users + fix)

### Capability Registration Verification

**Verified:** `suggest_hedges` capability IS implemented

**Location:** [backend/app/agents/financial_analyst.py:2832-2932](backend/app/agents/financial_analyst.py#L2832-L2932)

```python
async def financial_analyst_suggest_hedges(
    self,
    ctx: RequestCtx,
    state: Dict[str, Any],
    portfolio_id: Optional[str] = None,
    scenario_id: Optional[str] = None,
    scenario_result: Optional[Dict[str, Any]] = None,  # Pattern compatibility
    max_cost_bps: float = 20.0,
    **kwargs,
) -> Dict[str, Any]:
    """
    Suggest hedge instruments for a scenario stress test.

    Capability: financial_analyst.suggest_hedges
    (Consolidated from optimizer.suggest_hedges)
    """
```

**Status:** âœ… Method exists, consolidated from OptimizerAgent

**Action Required:** Verify capability routing in [backend/app/core/capability_mapping.py:56-62](backend/app/core/capability_mapping.py#L56-L62)

```python
"optimizer.suggest_hedges": {
    "target": "financial_analyst.suggest_hedges",
    "target_agent": "financial_analyst",
    "priority": 2,
    "risk_level": "medium",
    "dependencies": ["macro.run_scenario"],
}
```

**Testing Required:**
1. Pattern execution: `portfolio_scenario_analysis.json` step 6
2. Pattern execution: `portfolio_cycle_risk.json` step 4
3. Direct API call: `/api/patterns/execute` with scenario_id

---

## ðŸ—“ï¸ Week-by-Week Execution Plan

---

## WEEK 0: Foundation (Database Field Names + Integrity)

**Duration:** 3-5 days
**Goal:** Fix P0 database issues blocking all downstream work
**Team:** Database Team + Backend Lead
**Risk Level:** ðŸ”´ HIGH (database changes require downtime)

---

### Day 0: Preparation and Backup

#### Morning (9am - 12pm)

**Task 0.1: Production Freeze**
- [ ] Announce maintenance window (email all users)
- [ ] Enable read-only mode in application
- [ ] Disable all write endpoints (comment out in combined_server.py)
- [ ] Put up maintenance banner in UI

**Verification:**
```bash
# Test that writes are disabled
curl -X POST http://localhost:8000/api/transactions/create \
  -H "Content-Type: application/json" \
  -d '{"portfolio_id": "test", ...}'
# Should return: 503 Service Unavailable (maintenance mode)
```

**Task 0.2: Database Backup**
- [ ] Full pg_dump of production database
- [ ] Verify backup file integrity
- [ ] Test restore on staging environment
- [ ] Document restore procedure

**Commands:**
```bash
# Backup production
pg_dump -Fc dawsos_production > backup_20251104_production.dump

# Verify backup size
ls -lh backup_20251104_production.dump
# Should be ~500MB-1GB depending on data

# Test restore on staging
pg_restore -d dawsos_staging --clean --if-exists backup_20251104_production.dump

# Verify row counts match
psql -d dawsos_staging -c "SELECT
  (SELECT COUNT(*) FROM lots) as lots_count,
  (SELECT COUNT(*) FROM transactions) as txn_count,
  (SELECT COUNT(*) FROM portfolios) as portfolio_count;"
```

**Deliverables:**
- [x] Production freeze active
- [x] Backup file created and verified
- [x] Rollback procedure documented

---

#### Afternoon (1pm - 5pm)

**Task 0.3: Test Environment Setup**
- [ ] Clone production data to local test database
- [ ] Set up integration test suite
- [ ] Create test portfolios with known quantities
- [ ] Document expected values for validation

**Test Data Setup:**
```sql
-- Create test portfolio with known quantities
INSERT INTO portfolios (id, name, currency) VALUES
('test-portfolio-001', 'Test Portfolio Alpha', 'USD');

INSERT INTO lots (id, portfolio_id, security_id, symbol, quantity, qty_open, qty_original, cost_basis, cost_basis_per_share)
VALUES
  ('lot-001', 'test-portfolio-001', 'sec-aapl', 'AAPL', 100, 100, 100, 15000, 150),
  ('lot-002', 'test-portfolio-001', 'sec-msft', 'MSFT', 50, 50, 50, 17500, 350),
  ('lot-003', 'test-portfolio-001', 'sec-googl', 'GOOGL', 75, 75, 75, 22500, 300);

-- Expected total quantity: 225
-- Expected total cost basis: $55,000
```

**Validation Query:**
```sql
-- Test that data loaded correctly
SELECT
  COUNT(*) as lot_count,
  SUM(quantity) as total_quantity,
  SUM(qty_open) as total_qty_open,
  SUM(cost_basis) as total_cost
FROM lots
WHERE portfolio_id = 'test-portfolio-001';

-- Expected:
-- lot_count: 3
-- total_quantity: 225
-- total_qty_open: 225
-- total_cost: 55000
```

**Task 0.4: Monitoring Setup**
- [ ] Add database query logging (log all queries >100ms)
- [ ] Set up error aggregation (send all errors to Slack #alerts channel)
- [ ] Create alert thresholds (>5% error rate = page on-call)
- [ ] Test alerting (trigger test error)

**Deliverables:**
- [x] Test environment ready
- [x] Monitoring active
- [x] Day 0 complete

---

### Day 1-2: Database Field Name Standardization

**Goal:** Rename all database fields to standard names

---

#### Migration 014: Standardize Quantity Fields

**File:** `backend/db/migrations/014_standardize_field_names.sql`

**Strategy:** Add new columns, migrate data, keep old columns for transition period

```sql
-- Migration 014: Standardize Quantity Fields
-- Date: 2025-11-04
-- Purpose: Rename qty_open â†’ quantity_open, qty_original â†’ quantity_original
-- Strategy: Add new columns, migrate data, deprecate old columns (dual-write period)

BEGIN;

-- ============================================================================
-- Phase 1: Add New Columns
-- ============================================================================

ALTER TABLE lots
    ADD COLUMN quantity_open NUMERIC,
    ADD COLUMN quantity_original NUMERIC;

-- ============================================================================
-- Phase 2: Migrate Data
-- ============================================================================

UPDATE lots
SET
    quantity_open = qty_open,
    quantity_original = qty_original;

-- ============================================================================
-- Phase 3: Set Constraints
-- ============================================================================

ALTER TABLE lots
    ALTER COLUMN quantity_open SET NOT NULL,
    ALTER COLUMN quantity_original SET NOT NULL,
    ADD CONSTRAINT chk_lots_quantity_open_positive CHECK (quantity_open >= 0),
    ADD CONSTRAINT chk_lots_quantity_original_positive CHECK (quantity_original > 0),
    ADD CONSTRAINT chk_lots_quantity_open_lte_original CHECK (quantity_open <= quantity_original);

-- ============================================================================
-- Phase 4: Deprecate Old Columns (Keep for Transition)
-- ============================================================================

-- Rename old columns with _deprecated suffix
ALTER TABLE lots
    RENAME COLUMN qty_open TO qty_open_deprecated;

ALTER TABLE lots
    RENAME COLUMN qty_original TO qty_original_deprecated;

-- Add deprecation comments
COMMENT ON COLUMN lots.qty_open_deprecated IS
'âš ï¸ DEPRECATED: Use quantity_open instead. Will be removed in version 7.0 (Week 4)';

COMMENT ON COLUMN lots.qty_original_deprecated IS
'âš ï¸ DEPRECATED: Use quantity_original instead. Will be removed in version 7.0 (Week 4)';

-- ============================================================================
-- Phase 5: Update Indexes
-- ============================================================================

-- Recreate partial index with new column name
DROP INDEX IF EXISTS idx_lots_is_open;

CREATE INDEX idx_lots_open_positions
    ON lots(portfolio_id, security_id)
    WHERE quantity_open > 0;

-- ============================================================================
-- Phase 6: Validation
-- ============================================================================

-- Verify data integrity
DO $$
DECLARE
    mismatch_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO mismatch_count
    FROM lots
    WHERE quantity_open != qty_open_deprecated
       OR quantity_original != qty_original_deprecated;

    IF mismatch_count > 0 THEN
        RAISE EXCEPTION 'Data migration failed: % rows have mismatched values', mismatch_count;
    END IF;

    RAISE NOTICE 'Migration validation successful: All % rows migrated correctly',
        (SELECT COUNT(*) FROM lots);
END $$;

COMMIT;
```

**Rollback Script:**
```sql
-- Rollback 014: Revert to qty_open/qty_original
BEGIN;

-- Restore old column names
ALTER TABLE lots
    RENAME COLUMN qty_open_deprecated TO qty_open;

ALTER TABLE lots
    RENAME COLUMN qty_original_deprecated TO qty_original;

-- Drop new columns
ALTER TABLE lots
    DROP COLUMN IF EXISTS quantity_open,
    DROP COLUMN IF EXISTS quantity_original;

-- Restore old index
DROP INDEX IF EXISTS idx_lots_open_positions;
CREATE INDEX idx_lots_is_open ON lots(is_open) WHERE is_open = true;

COMMIT;
```

**Testing:**
```bash
# Run migration on test database
psql -d dawsos_test < backend/db/migrations/014_standardize_field_names.sql

# Verify schema
psql -d dawsos_test -c "\d lots"
# Should show:
#  quantity_open           | numeric | not null
#  quantity_original       | numeric | not null
#  qty_open_deprecated     | numeric |
#  qty_original_deprecated | numeric |

# Verify data migrated correctly
psql -d dawsos_test -c "
SELECT
  COUNT(*) as total_rows,
  COUNT(*) FILTER (WHERE quantity_open = qty_open_deprecated) as matching_open,
  COUNT(*) FILTER (WHERE quantity_original = qty_original_deprecated) as matching_original
FROM lots;
"
# Should show: total_rows = matching_open = matching_original

# Test constraints
psql -d dawsos_test -c "
INSERT INTO lots (
  id, portfolio_id, security_id, symbol,
  quantity, quantity_open, quantity_original,
  cost_basis, cost_basis_per_share
) VALUES (
  gen_random_uuid(), 'test-portfolio-001', 'sec-test', 'TEST',
  100, 150, 100,  -- âŒ quantity_open > quantity_original
  1000, 10
);
"
# Should fail with: CHECK constraint "chk_lots_quantity_open_lte_original" violated
```

---

#### Migration 015: Standardize Date Fields

**File:** `backend/db/migrations/015_standardize_date_fields.sql`

```sql
-- Migration 015: Standardize Date Fields
-- Date: 2025-11-04
-- Purpose: Rename all time-series date columns to asof_date
-- Strategy: Rename columns, rebuild indexes

BEGIN;

-- ============================================================================
-- Phase 1: Rename Columns
-- ============================================================================

-- portfolio_daily_values: valuation_date â†’ asof_date
ALTER TABLE portfolio_daily_values
    RENAME COLUMN valuation_date TO asof_date;

-- portfolio_cash_flows: flow_date â†’ asof_date
ALTER TABLE portfolio_cash_flows
    RENAME COLUMN flow_date TO asof_date;

-- macro_indicators: date â†’ asof_date
ALTER TABLE macro_indicators
    RENAME COLUMN date TO asof_date;

-- regime_history: date â†’ asof_date
ALTER TABLE regime_history
    RENAME COLUMN date TO asof_date;

-- ============================================================================
-- Phase 2: Rebuild Indexes
-- ============================================================================

-- portfolio_daily_values indexes
DROP INDEX IF EXISTS idx_portfolio_daily_values_date;
CREATE INDEX idx_portfolio_daily_values_asof
    ON portfolio_daily_values(portfolio_id, asof_date DESC);

-- macro_indicators indexes
DROP INDEX IF EXISTS idx_macro_indicators_date;
CREATE INDEX idx_macro_indicators_asof
    ON macro_indicators(indicator_name, asof_date DESC);

-- regime_history indexes
DROP INDEX IF EXISTS idx_regime_history_date;
CREATE INDEX idx_regime_history_asof
    ON regime_history(asof_date DESC);

-- ============================================================================
-- Phase 3: Update TimescaleDB Hypertable
-- ============================================================================

-- If portfolio_metrics is a hypertable, update time column
-- (Only if using TimescaleDB - skip if not)
SELECT set_chunk_time_interval('portfolio_metrics', INTERVAL '1 month', 'asof_date');

-- ============================================================================
-- Phase 4: Validation
-- ============================================================================

DO $$
BEGIN
    -- Verify all tables have asof_date column
    IF NOT EXISTS (
        SELECT 1 FROM information_schema.columns
        WHERE table_name = 'portfolio_daily_values' AND column_name = 'asof_date'
    ) THEN
        RAISE EXCEPTION 'portfolio_daily_values missing asof_date column';
    END IF;

    RAISE NOTICE 'Date field standardization successful';
END $$;

COMMIT;
```

**Code Updates Required (51 files):**

**Backend Services (14 files):**
```bash
# Files to update:
backend/app/services/metrics.py  # 8 occurrences
backend/app/services/currency_attribution.py  # 6 occurrences
backend/app/services/scenarios.py  # 5 occurrences
backend/app/services/risk.py  # 4 occurrences
# ... 10 more files
```

**Update Pattern:**
```python
# BEFORE:
query = """
SELECT twr, valuation_date
FROM portfolio_daily_values
WHERE portfolio_id = $1
ORDER BY valuation_date DESC
"""

# AFTER:
query = """
SELECT twr, asof_date
FROM portfolio_daily_values
WHERE portfolio_id = $1
ORDER BY asof_date DESC
"""
```

**Automated Search and Replace:**
```bash
# Find all occurrences
rg "valuation_date|flow_date" backend/app --type py

# Bulk replace (review first!)
fd -e py -x sd 'valuation_date' 'asof_date' {} \; backend/app/services/
fd -e py -x sd 'flow_date' 'asof_date' {} \; backend/app/services/
```

**Testing:**
```bash
# Test queries still work
psql -d dawsos_test -c "
SELECT portfolio_id, asof_date, nav, twr
FROM portfolio_daily_values
WHERE portfolio_id = 'test-portfolio-001'
ORDER BY asof_date DESC
LIMIT 5;
"
# Should return results with asof_date column

# Test Python code
python3 -c "
from backend.app.services.metrics import PerformanceCalculator
calc = PerformanceCalculator(db_connection)
result = calc.compute_twr('test-portfolio-001', lookback_days=30)
print(f'TWR: {result.twr_ytd}')
"
# Should work without errors
```

---

### Day 3: Database Integrity Fixes

#### Migration 016: Add Missing FK Constraints

**File:** `backend/db/migrations/016_add_missing_fk_constraints.sql`

```sql
-- Migration 016: Add Missing FK Constraints
-- Date: 2025-11-04
-- Purpose: Add foreign key constraints for referential integrity
-- Strategy: Clean orphaned records, add constraints

BEGIN;

-- ============================================================================
-- Phase 1: Identify Orphaned Records
-- ============================================================================

-- Check for orphaned lots (security_id not in securities)
CREATE TEMP TABLE orphaned_lots AS
SELECT l.*
FROM lots l
LEFT JOIN securities s ON l.security_id = s.id
WHERE s.id IS NULL;

-- Log orphaned records
DO $$
DECLARE
    orphan_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO orphan_count FROM orphaned_lots;

    IF orphan_count > 0 THEN
        RAISE WARNING 'Found % orphaned lots', orphan_count;

        -- Log details to audit log
        INSERT INTO audit_log (event_type, details, created_at)
        VALUES (
            'migration_orphaned_records',
            jsonb_build_object(
                'migration', '016_add_missing_fk_constraints',
                'table', 'lots',
                'count', orphan_count,
                'records', (SELECT jsonb_agg(row_to_json(orphaned_lots)) FROM orphaned_lots)
            ),
            NOW()
        );
    ELSE
        RAISE NOTICE 'No orphaned lots found';
    END IF;
END $$;

-- ============================================================================
-- Phase 2: Clean Orphaned Records
-- ============================================================================

-- Option A: Delete orphaned lots (if data is invalid)
-- DELETE FROM lots WHERE id IN (SELECT id FROM orphaned_lots);

-- Option B: Link to placeholder security (if data should be preserved)
-- First, create placeholder security
INSERT INTO securities (id, symbol, name, asset_class)
VALUES ('orphan-security-id', 'UNKNOWN', 'Unknown Security (Orphaned)', 'EQUITY')
ON CONFLICT (id) DO NOTHING;

-- Update orphaned lots to reference placeholder
UPDATE lots
SET security_id = 'orphan-security-id'
WHERE id IN (SELECT id FROM orphaned_lots);

-- ============================================================================
-- Phase 3: Add Foreign Key Constraints
-- ============================================================================

-- lots.security_id â†’ securities(id)
ALTER TABLE lots
    ADD CONSTRAINT fk_lots_security
    FOREIGN KEY (security_id)
    REFERENCES securities(id)
    ON DELETE RESTRICT;  -- Prevent accidental security deletion

-- lots.portfolio_id â†’ portfolios(id) (if not exists)
ALTER TABLE lots
    ADD CONSTRAINT fk_lots_portfolio
    FOREIGN KEY (portfolio_id)
    REFERENCES portfolios(id)
    ON DELETE CASCADE;  -- Delete lots when portfolio deleted

-- transactions.security_id â†’ securities(id)
-- (Allow NULL for fee transactions)
ALTER TABLE transactions
    ADD CONSTRAINT fk_transactions_security
    FOREIGN KEY (security_id)
    REFERENCES securities(id)
    ON DELETE SET NULL;  -- Set to NULL if security deleted

-- transactions.portfolio_id â†’ portfolios(id)
ALTER TABLE transactions
    ADD CONSTRAINT fk_transactions_portfolio
    FOREIGN KEY (portfolio_id)
    REFERENCES portfolios(id)
    ON DELETE CASCADE;

-- position_factor_betas.security_id â†’ securities(id)
ALTER TABLE position_factor_betas
    ADD CONSTRAINT fk_position_betas_security
    FOREIGN KEY (security_id)
    REFERENCES securities(id)
    ON DELETE CASCADE;  -- OK to delete betas if security deleted

-- ============================================================================
-- Phase 4: Validation
-- ============================================================================

-- Verify all FK constraints created
DO $$
DECLARE
    fk_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO fk_count
    FROM information_schema.table_constraints
    WHERE constraint_type = 'FOREIGN KEY'
      AND constraint_name IN (
          'fk_lots_security',
          'fk_lots_portfolio',
          'fk_transactions_security',
          'fk_transactions_portfolio',
          'fk_position_betas_security'
      );

    IF fk_count < 5 THEN
        RAISE EXCEPTION 'Not all FK constraints created (expected 5, got %)', fk_count;
    END IF;

    RAISE NOTICE 'All FK constraints created successfully';
END $$;

-- Test that orphaned records cannot be created
DO $$
BEGIN
    -- Try to insert lot with invalid security_id
    INSERT INTO lots (
        id, portfolio_id, security_id, symbol,
        quantity, quantity_open, quantity_original,
        cost_basis, cost_basis_per_share
    ) VALUES (
        gen_random_uuid(), 'test-portfolio-001', 'invalid-security-id', 'TEST',
        100, 100, 100, 1000, 10
    );

    RAISE EXCEPTION 'FK constraint did not prevent orphaned record';
EXCEPTION
    WHEN foreign_key_violation THEN
        RAISE NOTICE 'FK constraint working correctly (orphaned record prevented)';
END $$;

COMMIT;
```

**Testing:**
```bash
# Run migration
psql -d dawsos_test < backend/db/migrations/016_add_missing_fk_constraints.sql

# Verify FK constraints exist
psql -d dawsos_test -c "
SELECT
  tc.constraint_name,
  tc.table_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY'
  AND tc.table_name IN ('lots', 'transactions', 'position_factor_betas');
"
# Should show 5 FK constraints

# Test that orphaned records are prevented
psql -d dawsos_test -c "
INSERT INTO lots (
  id, portfolio_id, security_id, symbol,
  quantity, quantity_open, quantity_original,
  cost_basis, cost_basis_per_share
) VALUES (
  gen_random_uuid(), 'test-portfolio-001', 'nonexistent-security', 'FAKE',
  100, 100, 100, 1000, 10
);
"
# Should fail with: ERROR:  insert or update on table "lots" violates foreign key constraint "fk_lots_security"
```

---

#### Migration 017: Fix Duplicate Table Definitions

**File:** `backend/db/migrations/017_remove_duplicate_tables.sql`

```sql
-- Migration 017: Remove Duplicate Table Definitions
-- Date: 2025-11-04
-- Purpose: Fix duplicate position_factor_betas and scenario_shocks tables
-- Issue: Tables defined in BOTH schema/ and migration 009

BEGIN;

-- ============================================================================
-- Phase 1: Identify Which Version is Populated
-- ============================================================================

-- Check if position_factor_betas has data
DO $$
DECLARE
    row_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO row_count FROM position_factor_betas;
    RAISE NOTICE 'position_factor_betas has % rows', row_count;
END $$;

-- Check if scenario_shocks has data
DO $$
DECLARE
    row_count INTEGER;
BEGIN
    SELECT COUNT(*) INTO row_count FROM scenario_shocks;
    RAISE NOTICE 'scenario_shocks has % rows', row_count;
END $$;

-- ============================================================================
-- Phase 2: Reconcile Table Structure
-- ============================================================================

-- position_factor_betas: Use schema version (composite PK)
-- Drop and recreate if structure doesn't match

-- First, backup existing data
CREATE TEMP TABLE position_factor_betas_backup AS
SELECT * FROM position_factor_betas;

-- Drop existing table
DROP TABLE IF EXISTS position_factor_betas CASCADE;

-- Recreate with schema definition (from scenario_factor_tables.sql)
CREATE TABLE position_factor_betas (
    portfolio_id UUID NOT NULL,
    security_id UUID NOT NULL,
    factor_name VARCHAR(50) NOT NULL,
    beta NUMERIC(10, 4) NOT NULL,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (portfolio_id, security_id, factor_name),
    CONSTRAINT fk_position_betas_portfolio
        FOREIGN KEY (portfolio_id) REFERENCES portfolios(id) ON DELETE CASCADE,
    CONSTRAINT fk_position_betas_security
        FOREIGN KEY (security_id) REFERENCES securities(id) ON DELETE CASCADE,
    CONSTRAINT chk_factor_name CHECK (
        factor_name IN (
            'equity_beta', 'duration', 'credit_spread_beta',
            'real_rate_beta', 'inflation_beta', 'vix_beta',
            'momentum', 'value', 'quality', 'size', 'low_vol',
            'dividend_yield', 'eps_growth', 'roe', 'leverage'
        )
    )
);

-- Restore data if any
INSERT INTO position_factor_betas
SELECT portfolio_id, security_id, factor_name, beta, updated_at
FROM position_factor_betas_backup
ON CONFLICT (portfolio_id, security_id, factor_name) DO UPDATE
SET beta = EXCLUDED.beta, updated_at = EXCLUDED.updated_at;

-- ============================================================================
-- Phase 3: Do Same for scenario_shocks
-- ============================================================================

-- Backup
CREATE TEMP TABLE scenario_shocks_backup AS
SELECT * FROM scenario_shocks;

-- Drop
DROP TABLE IF EXISTS scenario_shocks CASCADE;

-- Recreate with schema definition
CREATE TABLE scenario_shocks (
    scenario_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    scenario_name VARCHAR(100) NOT NULL UNIQUE,
    scenario_category VARCHAR(50) NOT NULL,
    shock_definition JSONB NOT NULL,
    description TEXT,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    CONSTRAINT chk_scenario_category CHECK (
        scenario_category IN ('deleveraging', 'stagflation', 'regime_shift', 'tail_risk', 'custom')
    )
);

-- Restore data
INSERT INTO scenario_shocks
SELECT scenario_id, scenario_name, scenario_category, shock_definition, description, created_at
FROM scenario_shocks_backup
ON CONFLICT (scenario_id) DO NOTHING;

-- ============================================================================
-- Phase 4: Update Migration File
-- ============================================================================

-- Manual action required: Edit 009_add_scenario_dar_tables.sql
-- Remove CREATE TABLE statements for position_factor_betas and scenario_shocks
-- Keep only INSERT statements for seed data

RAISE NOTICE 'Migration 017 complete. Manual action required:';
RAISE NOTICE '1. Edit backend/db/migrations/009_add_scenario_dar_tables.sql';
RAISE NOTICE '2. Remove CREATE TABLE position_factor_betas (lines 70-97)';
RAISE NOTICE '3. Remove CREATE TABLE scenario_shocks (lines 105-135)';
RAISE NOTICE '4. Keep only INSERT statements for seed data';

COMMIT;
```

**Manual Cleanup:**
```bash
# Edit migration file
vim backend/db/migrations/009_add_scenario_dar_tables.sql

# Find and delete these sections:
# - Lines 70-97: CREATE TABLE position_factor_betas
# - Lines 105-135: CREATE TABLE scenario_shocks

# Keep only:
# - Lines 1-50: Comments and other tables
# - Lines 150+: INSERT statements for seed data
```

---

#### Migration 018: Renumber Conflicting Migrations

**Manual Task - No SQL File**

**Files to Rename:**
```bash
# Current state:
backend/db/migrations/009_jwt_auth.sql  # â† Conflict with 009_add_scenario_dar_tables.sql
backend/db/migrations/010_fix_audit_log_schema.sql  # â† Conflict with 010_add_users_and_audit_log.sql

# Target state:
backend/db/migrations/018_jwt_auth.sql  # Moved to end
backend/db/migrations/019_fix_audit_log_schema.sql  # Moved to end
```

**Commands:**
```bash
cd backend/db/migrations/

# Rename conflicting files
mv 009_jwt_auth.sql 018_jwt_auth.sql
mv 010_fix_audit_log_schema.sql 019_fix_audit_log_schema.sql

# Update migration tracker
psql -d dawsos_production -c "
UPDATE schema_migrations
SET migration_number = 18
WHERE migration_file = '009_jwt_auth.sql';

UPDATE schema_migrations
SET migration_number = 19
WHERE migration_file = '010_fix_audit_log_schema.sql';
"
```

---

### Week 0 Completion Checklist

**Database Migrations:**
- [ ] Migration 014: Field names standardized (qty_open â†’ quantity_open)
- [ ] Migration 015: Date fields standardized (all â†’ asof_date)
- [ ] Migration 016: FK constraints added
- [ ] Migration 017: Duplicate tables fixed
- [ ] Migration 018: Migrations renumbered

**Code Updates:**
- [ ] 51 backend files updated for new field names
- [ ] All SQL queries tested
- [ ] All Python services tested

**Validation:**
- [ ] All test portfolios validate correctly
- [ ] Row counts match before/after
- [ ] No orphaned records exist
- [ ] FK constraints prevent invalid data

**Production Deployment:**
- [ ] Migrations run on production
- [ ] Validation queries pass
- [ ] Application restarted
- [ ] Read-only mode disabled

---

## WEEK 1-2: Pattern System Refactoring

**Duration:** 10 days
**Goal:** Simplify pattern system based on standardized field names
**Team:** Backend Team + Frontend Team
**Risk Level:** ðŸŸ¡ MEDIUM (changes to pattern system)

**Dependencies:** âœ… Week 0 complete (field names standardized)

---

### Week 1-2 Overview

**What We're Fixing:**
1. Pattern JSON files reference old field names (qty, value)
2. UI patternRegistry has 46 dataPath mappings with inconsistent field expectations
3. Backend pattern responses need field name validation

**Impact:**
- 13 pattern JSON files need field name updates
- 46 dataPath mappings in UI need verification
- Backend pattern orchestrator needs validation layer

---

### Day 4-5: Pattern JSON Updates

**Goal:** Update all 13 pattern JSON files to use standardized field names

**Pattern Files to Update:**
```
backend/patterns/
â”œâ”€â”€ portfolio_overview.json  â† line 172: "quantity" (correct)
â”œâ”€â”€ holding_deep_dive.json  â† needs review
â”œâ”€â”€ policy_rebalance.json  â† needs review
â”œâ”€â”€ portfolio_scenario_analysis.json  â† needs review
â”œâ”€â”€ portfolio_cycle_risk.json  â† needs review
â”œâ”€â”€ macro_cycles_overview.json  â† no qty fields
â”œâ”€â”€ cycle_deleveraging_scenarios.json  â† no qty fields
â”œâ”€â”€ buffett_checklist.json  â† no qty fields
â”œâ”€â”€ macro_trend_monitor.json  â† no qty fields
â”œâ”€â”€ news_impact_analysis.json  â† needs review
â”œâ”€â”€ portfolio_macro_overview.json  â† needs review
â”œâ”€â”€ corporate_actions_upcoming.json  â† needs review
â””â”€â”€ export_portfolio_report.json  â† needs review
```

**Task 1: Audit All Pattern Field References**

```bash
# Find all field references in patterns
grep -rn "\"field\":" backend/patterns/ | grep -E "qty|quantity|value|market_value"

# Example output:
# portfolio_overview.json:172:        {"field": "quantity", "header": "Qty"},
# portfolio_overview.json:173:        {"field": "market_value", "header": "Value"},
# holding_deep_dive.json:85:        {"field": "qty", "header": "Quantity"},  â† WRONG
```

**Task 2: Create Standardization Script**

**File:** `scripts/standardize_pattern_fields.py`

```python
#!/usr/bin/env python3
"""
Standardize field names in pattern JSON files.

Updates:
- qty â†’ quantity
- value â†’ market_value (when ambiguous)
- val â†’ market_value
"""

import json
from pathlib import Path
from typing import Dict, Any, List

def standardize_field_name(field: str) -> str:
    """Standardize a field name."""
    mapping = {
        'qty': 'quantity',
        'value': 'market_value',  # Ambiguous, assume market value
        'val': 'market_value',
    }
    return mapping.get(field, field)

def standardize_pattern(pattern: Dict[str, Any]) -> Dict[str, Any]:
    """Standardize field names in a pattern."""

    # Update presentation.*.columns[].field
    if 'presentation' in pattern:
        for panel_id, panel_config in pattern['presentation'].items():
            if 'columns' in panel_config:
                for column in panel_config['columns']:
                    if 'field' in column:
                        old_field = column['field']
                        new_field = standardize_field_name(old_field)
                        if old_field != new_field:
                            print(f"  {panel_id}: {old_field} â†’ {new_field}")
                            column['field'] = new_field

    return pattern

def main():
    """Standardize all pattern files."""
    pattern_dir = Path('backend/patterns')

    for pattern_file in pattern_dir.glob('*.json'):
        print(f"\nProcessing {pattern_file.name}...")

        with open(pattern_file) as f:
            pattern = json.load(f)

        updated_pattern = standardize_pattern(pattern)

        with open(pattern_file, 'w') as f:
            json.dump(updated_pattern, f, indent=2)

        print(f"  âœ“ {pattern_file.name} updated")

if __name__ == '__main__':
    main()
```

**Run Standardization:**
```bash
python3 scripts/standardize_pattern_fields.py
```

**Task 3: Validate Pattern Files**

```bash
# Verify no old field names remain
grep -rn "\"field\":" backend/patterns/ | grep -E "\"qty\"|\"val\"[^u]"
# Should return: no matches

# Verify all patterns are valid JSON
for file in backend/patterns/*.json; do
  echo "Validating $file..."
  python3 -m json.tool "$file" > /dev/null && echo "  âœ“ Valid" || echo "  âœ— Invalid"
done
```

---

### Day 6-8: Backend Pattern Response Validation

**Goal:** Add Pydantic schema validation for pattern responses

**Task 1: Create Pattern Response Schemas**

**File:** `backend/app/schemas/pattern_responses.py`

```python
"""
Pattern Response Schemas

Pydantic models for validating pattern execution responses.
Ensures field names match UI expectations.
"""

from pydantic import BaseModel, Field, validator
from typing import List, Dict, Any, Optional
from decimal import Decimal
from datetime import date
from uuid import UUID

# ============================================================================
# Common Field Models
# ============================================================================

class Position(BaseModel):
    """Standard position model for patterns."""
    security_id: UUID
    symbol: str
    quantity: Decimal = Field(..., gt=0, description="Position quantity (standardized)")
    market_value: Decimal = Field(..., ge=0, description="Market value (standardized)")
    cost_basis: Decimal = Field(..., ge=0)
    unrealized_pnl: Decimal
    unrealized_pnl_pct: Decimal
    weight: Optional[Decimal] = None
    dividend_safety: Optional[int] = None
    moat_strength: Optional[int] = None

    class Config:
        frozen = True

    @validator('quantity')
    def quantity_positive(cls, v):
        if v <= 0:
            raise ValueError('quantity must be positive')
        return v

    @validator('weight')
    def weight_valid(cls, v):
        if v is not None and (v < 0 or v > 1):
            raise ValueError('weight must be between 0 and 1')
        return v

class PerformanceMetrics(BaseModel):
    """Performance metrics for portfolio overview."""
    twr_ytd: Decimal
    twr_1y: Decimal
    volatility_30d: Decimal
    sharpe_ratio: Decimal
    max_drawdown: Decimal

    @validator('sharpe_ratio')
    def sharpe_reasonable(cls, v):
        if v < -5 or v > 10:
            raise ValueError('Sharpe ratio out of reasonable range')
        return v

# ============================================================================
# Pattern-Specific Response Models
# ============================================================================

class PortfolioOverviewResponse(BaseModel):
    """Response for portfolio_overview pattern."""
    perf_metrics: PerformanceMetrics
    currency_attr: Dict[str, Any]
    valued_positions: Dict[str, List[Position]]
    sector_allocation: Dict[str, Any]
    historical_nav: Dict[str, Any]
    _trace: Dict[str, Any]
    _metadata: Dict[str, Any]

class HoldingDeepDiveResponse(BaseModel):
    """Response for holding_deep_dive pattern."""
    position: Position
    fundamentals: Dict[str, Any]
    transaction_history: List[Dict[str, Any]]
    _trace: Dict[str, Any]
    _metadata: Dict[str, Any]

# ============================================================================
# Pattern Response Validator
# ============================================================================

class PatternResponseValidator:
    """Validates pattern responses against schemas."""

    PATTERN_SCHEMAS = {
        'portfolio_overview': PortfolioOverviewResponse,
        'holding_deep_dive': HoldingDeepDiveResponse,
        # ... add more patterns
    }

    @classmethod
    def validate(cls, pattern_id: str, response: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate pattern response.

        Args:
            pattern_id: Pattern ID
            response: Response dict from pattern execution

        Returns:
            Validated response dict

        Raises:
            ValidationError: If response doesn't match schema
        """
        schema = cls.PATTERN_SCHEMAS.get(pattern_id)
        if not schema:
            # No schema defined, return as-is
            return response

        # Validate with Pydantic
        validated = schema(**response)
        return validated.dict()
```

**Task 2: Integrate Validation into PatternOrchestrator**

**File:** `backend/app/core/pattern_orchestrator.py`

**Update the run_pattern method:**

```python
# Add import
from app.schemas.pattern_responses import PatternResponseValidator

class PatternOrchestrator:
    # ... existing code ...

    async def run_pattern(
        self,
        pattern_id: str,
        ctx: RequestCtx,
        inputs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Execute pattern with validation."""

        # ... existing execution code ...

        # Build response
        result = {}
        for key, value in state.items():
            result[key] = value

        # Add trace and metadata
        result["_trace"] = trace.to_dict()
        result["_metadata"] = {
            "pattern_id": pattern_id,
            "execution_time": time.time() - start_time,
            "pricing_pack_id": ctx.pricing_pack_id,
        }

        # âœ¨ NEW: Validate response
        try:
            validated_result = PatternResponseValidator.validate(pattern_id, result)
            logger.info(f"Pattern {pattern_id} response validated successfully")
            return validated_result
        except ValidationError as e:
            logger.error(f"Pattern {pattern_id} response validation failed: {e}")
            # Return original result but add validation warning to metadata
            result["_metadata"]["validation_warning"] = str(e)
            return result
```

**Task 3: Test Pattern Validation**

```python
# Test script: tests/integration/test_pattern_validation.py

import pytest
from app.core.pattern_orchestrator import PatternOrchestrator
from app.core.types import RequestCtx

@pytest.mark.asyncio
async def test_portfolio_overview_response_valid():
    """Test that portfolio_overview returns valid response."""

    orchestrator = PatternOrchestrator(agent_runtime, db)

    ctx = RequestCtx(
        pricing_pack_id="PP_latest",
        portfolio_id="test-portfolio-001"
    )

    result = await orchestrator.run_pattern(
        "portfolio_overview",
        ctx,
        {"portfolio_id": "test-portfolio-001"}
    )

    # Should have all expected fields
    assert "perf_metrics" in result
    assert "valued_positions" in result
    assert "_trace" in result
    assert "_metadata" in result

    # Positions should have standardized field names
    positions = result["valued_positions"]["positions"]
    if positions:
        pos = positions[0]
        assert "quantity" in pos  # Not qty!
        assert "market_value" in pos  # Not value!
        assert "qty" not in pos
        assert pos["quantity"] > 0
```

---

### Day 9-10: UI PatternRegistry Updates

**Goal:** Update UI patternRegistry to match backend field names

**Task 1: Audit PatternRegistry DataPaths**

**File:** `full_ui.html` (lines 2832-3451)

**Current DataPath Mappings (46 total):**
```javascript
const patternRegistry = {
    'portfolio_overview': {
        panels: [
            { dataPath: 'perf_metrics' },  // âœ“ OK
            { dataPath: 'valued_positions.positions' },  // âœ“ OK
            { dataPath: 'currency_attr' },  // âœ“ OK
            { dataPath: 'sector_allocation' },  // âœ“ OK
            { dataPath: 'historical_nav' }  // âœ“ OK
        ]
    },
    'portfolio_scenario_analysis': {
        panels: [
            { dataPath: 'scenario_result' },  // âœ“ OK
            { dataPath: 'scenario_result.position_deltas' },  // âš ï¸ Check field names
            { dataPath: 'hedge_suggestions.suggestions' }  // âœ“ OK (suggest_hedges fixed)
        ]
    },
    // ... 11 more patterns
}
```

**Task 2: Create DataPath Validation Script**

**File:** `scripts/validate_datapath.py`

```python
#!/usr/bin/env python3
"""
Validate UI dataPaths match backend pattern responses.

Compares:
- PatternRegistry dataPaths in full_ui.html
- Pattern JSON outputs in backend/patterns/*.json
- Backend response schemas in backend/app/schemas/pattern_responses.py
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Set

def extract_datapaths_from_ui(ui_file: Path) -> Dict[str, List[str]]:
    """Extract dataPaths from patternRegistry in UI."""
    with open(ui_file) as f:
        content = f.read()

    # Find patternRegistry object
    match = re.search(r'const patternRegistry = \{(.+?)\};', content, re.DOTALL)
    if not match:
        raise ValueError("patternRegistry not found in UI")

    registry_str = match.group(1)

    # Extract dataPaths
    datapaths = {}
    for pattern_match in re.finditer(r"'([^']+)':\s*\{", registry_str):
        pattern_id = pattern_match.group(1)
        datapaths[pattern_id] = []

        # Find all dataPaths for this pattern
        for path_match in re.finditer(r"dataPath:\s*'([^']+)'", registry_str):
            datapaths[pattern_id].append(path_match.group(1))

    return datapaths

def extract_outputs_from_patterns(pattern_dir: Path) -> Dict[str, Set[str]]:
    """Extract outputs from pattern JSON files."""
    outputs = {}

    for pattern_file in pattern_dir.glob('*.json'):
        with open(pattern_file) as f:
            pattern = json.load(f)

        pattern_id = pattern['id']
        outputs[pattern_id] = set(pattern.get('outputs', []))

    return outputs

def validate_datapaths(
    ui_datapaths: Dict[str, List[str]],
    pattern_outputs: Dict[str, Set[str]]
) -> List[str]:
    """Validate that UI dataPaths match pattern outputs."""
    issues = []

    for pattern_id, paths in ui_datapaths.items():
        if pattern_id not in pattern_outputs:
            issues.append(f"Pattern {pattern_id} not found in backend/patterns/")
            continue

        outputs = pattern_outputs[pattern_id]

        for path in paths:
            # Handle nested paths (e.g., "valued_positions.positions")
            root = path.split('.')[0]

            if root not in outputs:
                issues.append(
                    f"Pattern {pattern_id}: dataPath '{path}' not in outputs {outputs}"
                )

    return issues

def main():
    """Validate dataPaths."""
    ui_file = Path('full_ui.html')
    pattern_dir = Path('backend/patterns')

    print("Extracting dataPaths from UI...")
    ui_datapaths = extract_datapaths_from_ui(ui_file)

    print("Extracting outputs from patterns...")
    pattern_outputs = extract_outputs_from_patterns(pattern_dir)

    print("Validating dataPaths...")
    issues = validate_datapaths(ui_datapaths, pattern_outputs)

    if issues:
        print(f"\nâŒ Found {len(issues)} issues:")
        for issue in issues:
            print(f"  - {issue}")
        return 1
    else:
        print("\nâœ… All dataPaths valid")
        return 0

if __name__ == '__main__':
    exit(main())
```

**Run Validation:**
```bash
python3 scripts/validate_datapath.py
```

**Task 3: Fix DataPath Issues**

Based on validation results, update patternRegistry in full_ui.html:

```javascript
// Example fix:
// BEFORE:
{
    dataPath: 'scenario_result.position_deltas',
    columns: [
        { field: 'symbol', header: 'Symbol' },
        { field: 'qty', header: 'Quantity' },  // âŒ OLD
        { field: 'value', header: 'Value' }  // âŒ OLD
    ]
}

// AFTER:
{
    dataPath: 'scenario_result.position_deltas',
    columns: [
        { field: 'symbol', header: 'Symbol' },
        { field: 'quantity', header: 'Quantity' },  // âœ… NEW
        { field: 'market_value', header: 'Value' }  // âœ… NEW
    ]
}
```

---

### Week 1-2 Completion Checklist

**Pattern JSON Files:**
- [ ] All 13 patterns audited
- [ ] Field names standardized (quantity, market_value)
- [ ] JSON validation passes

**Backend Validation:**
- [ ] Pydantic schemas created
- [ ] Pattern orchestrator validation integrated
- [ ] All patterns tested with validation

**UI Updates:**
- [ ] PatternRegistry dataPaths validated
- [ ] Field names updated in UI
- [ ] DataPath validation script passes

**Integration Testing:**
- [ ] All 13 patterns execute successfully
- [ ] UI displays all data correctly
- [ ] No console errors

---

## WEEK 3: Complete System Fixes

**Duration:** 5 days
**Goal:** Complete missing pieces and system-wide fixes
**Team:** Full team
**Risk Level:** ðŸŸ¢ LOW (cleanup and verification)

**Dependencies:** âœ… Week 0 + Week 1-2 complete

---

### Day 11-12: Capability Verification and Testing

**Task 1: Verify suggest_hedges Registration**

**File:** `backend/app/core/capability_mapping.py`

Already correct (lines 56-62):
```python
"optimizer.suggest_hedges": {
    "target": "financial_analyst.suggest_hedges",
    "target_agent": "financial_analyst",
    "priority": 2,
    "risk_level": "medium",
    "dependencies": ["macro.run_scenario"],
}
```

**Task 2: Test suggest_hedges Capability**

```bash
# Test direct API call
curl -X POST http://localhost:8000/api/patterns/execute \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "pattern_id": "portfolio_scenario_analysis",
    "inputs": {
      "portfolio_id": "test-portfolio-001",
      "scenario_id": "rates_up"
    },
    "ctx": {
      "pricing_pack_id": "PP_latest"
    }
  }'

# Should return:
# {
#   "scenario_result": {...},
#   "hedge_suggestions": {
#     "hedges": [...],  â† From suggest_hedges
#     "total_notional": 50000
#   },
#   "_trace": {...}
# }
```

**Task 3: Test portfolio_cycle_risk Pattern**

```bash
curl -X POST http://localhost:8000/api/patterns/execute \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{
    "pattern_id": "portfolio_cycle_risk",
    "inputs": {
      "portfolio_id": "test-portfolio-001"
    }
  }'

# Should execute step 4 (suggest_deleveraging_hedges) without errors
```

---

### Day 13: Auth Token Refresh Implementation

**Goal:** Add axios interceptor for token refresh in UI

**Task 1: Add Token Refresh Logic**

**File:** `full_ui.html`

**Add after axios initialization (around line 5600):**

```javascript
// Token refresh interceptor
let isRefreshing = false;
let failedQueue = [];

const processQueue = (error, token = null) => {
    failedQueue.forEach(prom => {
        if (error) {
            prom.reject(error);
        } else {
            prom.resolve(token);
        }
    });

    failedQueue = [];
};

axios.interceptors.response.use(
    response => response,
    async error => {
        const originalRequest = error.config;

        // If 401 and not already retrying
        if (error.response?.status === 401 && !originalRequest._retry) {
            if (isRefreshing) {
                // Another request is already refreshing, queue this one
                return new Promise((resolve, reject) => {
                    failedQueue.push({resolve, reject});
                }).then(token => {
                    originalRequest.headers['Authorization'] = 'Bearer ' + token;
                    return axios(originalRequest);
                }).catch(err => {
                    return Promise.reject(err);
                });
            }

            originalRequest._retry = true;
            isRefreshing = true;

            try {
                // Get current token
                const currentToken = localStorage.getItem('token');

                // Call refresh endpoint
                const response = await axios.post('/api/auth/refresh', {}, {
                    headers: { 'Authorization': 'Bearer ' + currentToken }
                });

                const newToken = response.data.token;

                // Save new token
                localStorage.setItem('token', newToken);
                axios.defaults.headers.common['Authorization'] = 'Bearer ' + newToken;

                // Process queued requests
                processQueue(null, newToken);

                // Retry original request
                originalRequest.headers['Authorization'] = 'Bearer ' + newToken;
                return axios(originalRequest);

            } catch (refreshError) {
                processQueue(refreshError, null);

                // Refresh failed - redirect to login
                localStorage.removeItem('token');
                window.location.href = '/';
                return Promise.reject(refreshError);

            } finally {
                isRefreshing = false;
            }
        }

        return Promise.reject(error);
    }
);
```

**Task 2: Test Token Refresh**

```bash
# Simulate expired token
# 1. Login to get token
# 2. Wait 24 hours (or manually expire token in backend)
# 3. Make API call
# 4. Verify token is automatically refreshed
# 5. Verify original request completes

# Manual test:
# 1. Login
# 2. In browser console: localStorage.setItem('token', 'expired-token')
# 3. Navigate to dashboard
# 4. Should auto-refresh and load dashboard (not redirect to login)
```

---

### Day 14-15: Error Handling Improvements

**Task 1: Create Error Taxonomy**

**File:** `backend/app/core/errors.py`

```python
"""
Error Taxonomy for DawsOS

Structured error responses with user-actionable suggestions.
"""

from enum import Enum
from typing import Optional

class ErrorCode(Enum):
    """Error codes for structured error responses."""

    # Authentication (1xxx)
    INVALID_CREDENTIALS = "AUTH_1001"
    TOKEN_EXPIRED = "AUTH_1002"
    TOKEN_INVALID = "AUTH_1003"

    # Validation (2xxx)
    INVALID_PORTFOLIO_ID = "VALIDATION_2001"
    INVALID_SECURITY_ID = "VALIDATION_2002"
    INVALID_QUANTITY = "VALIDATION_2003"
    INVALID_DATE_RANGE = "VALIDATION_2004"

    # Data Not Found (3xxx)
    PORTFOLIO_NOT_FOUND = "NOT_FOUND_3001"
    SECURITY_NOT_FOUND = "NOT_FOUND_3002"
    PRICING_PACK_NOT_FOUND = "NOT_FOUND_3003"

    # Business Logic (4xxx)
    INSUFFICIENT_BALANCE = "BUSINESS_4001"
    POSITION_NOT_OPEN = "BUSINESS_4002"
    PATTERN_EXECUTION_FAILED = "BUSINESS_4003"
    CAPABILITY_NOT_FOUND = "BUSINESS_4004"

    # System (5xxx)
    DATABASE_ERROR = "SYSTEM_5001"
    EXTERNAL_API_ERROR = "SYSTEM_5002"
    INTERNAL_ERROR = "SYSTEM_5003"

class DawsOSError(Exception):
    """Base exception for DawsOS errors."""

    def __init__(
        self,
        code: ErrorCode,
        message: str,
        suggestion: Optional[str] = None,
        details: Optional[dict] = None
    ):
        self.code = code
        self.message = message
        self.suggestion = suggestion
        self.details = details or {}
        super().__init__(message)

    def to_dict(self) -> dict:
        """Convert to structured error response."""
        return {
            "error": {
                "code": self.code.value,
                "message": self.message,
                "suggestion": self.suggestion,
                "details": self.details
            }
        }

# Convenience constructors
class ValidationError(DawsOSError):
    """Validation error."""
    pass

class NotFoundError(DawsOSError):
    """Resource not found error."""
    pass

class BusinessLogicError(DawsOSError):
    """Business logic error."""
    pass
```

**Task 2: Update Exception Handlers**

**File:** `combined_server.py`

```python
from backend.app.core.errors import DawsOSError, ErrorCode

# Update exception handler
@app.exception_handler(DawsOSError)
async def dawsos_error_handler(request: Request, exc: DawsOSError):
    """Handle structured DawsOS errors."""
    return JSONResponse(
        status_code=400,  # or appropriate code
        content=exc.to_dict()
    )

# Update generic exception handler
@app.exception_handler(Exception)
async def generic_exception_handler(request: Request, exc: Exception):
    """Handle unexpected errors."""
    logger.error(f"Unexpected error: {exc}", exc_info=True)

    error = DawsOSError(
        code=ErrorCode.INTERNAL_ERROR,
        message="An unexpected error occurred",
        suggestion="Please try again. If the problem persists, contact support.",
        details={"exception_type": type(exc).__name__}
    )

    return JSONResponse(
        status_code=500,
        content=error.to_dict()
    )
```

**Task 3: Update UI Error Display**

**File:** `full_ui.html`

```javascript
// Update error display function
function displayError(error) {
    const errorData = error.response?.data?.error || {};

    const errorCode = errorData.code || 'UNKNOWN';
    const errorMessage = errorData.message || 'An error occurred';
    const errorSuggestion = errorData.suggestion || 'Please try again';

    // Create error element
    const errorElement = e('div', { className: 'error-banner' },
        e('div', { className: 'error-header' },
            e('span', { className: 'error-code' }, errorCode),
            e('span', { className: 'error-message' }, errorMessage)
        ),
        e('div', { className: 'error-suggestion' }, errorSuggestion),
        e('button', {
            className: 'error-dismiss',
            onClick: () => errorElement.remove()
        }, 'Dismiss')
    );

    // Append to page
    document.querySelector('.main-content').prepend(errorElement);

    // Auto-dismiss after 10 seconds
    setTimeout(() => errorElement.remove(), 10000);
}
```

---

### Week 3 Completion Checklist

**Capability Verification:**
- [ ] suggest_hedges tested with portfolio_scenario_analysis
- [ ] suggest_deleveraging_hedges tested with portfolio_cycle_risk
- [ ] All capability mappings verified

**Auth Improvements:**
- [ ] Token refresh interceptor added
- [ ] Token refresh tested (expired token scenario)
- [ ] Login flow tested

**Error Handling:**
- [ ] Error taxonomy created
- [ ] Exception handlers updated
- [ ] UI error display improved
- [ ] User-actionable suggestions added

---

## WEEK 4: End-to-End Validation & Production Rollout

**Duration:** 5 days
**Goal:** Validate entire system and deploy to production
**Team:** Full team + QA
**Risk Level:** ðŸ”´ HIGH (production deployment)

**Dependencies:** âœ… Week 0-3 complete

---

### Day 16-17: Comprehensive Testing

**Task 1: Run Pattern Validation Suite**

**File:** `tests/integration/test_all_patterns.py`

```python
#!/usr/bin/env python3
"""
Comprehensive pattern validation test suite.

Tests all 13 patterns end-to-end with standardized field names.
"""

import pytest
import asyncio
from pathlib import Path
import json

from app.core.agent_runtime import AgentRuntime
from app.core.pattern_orchestrator import PatternOrchestrator
from app.core.types import RequestCtx
from app.db.connection import get_db_connection

# Test portfolios
TEST_PORTFOLIOS = [
    "test-portfolio-001",
    "test-portfolio-002",
    "test-portfolio-003"
]

# Pattern test cases
PATTERN_TESTS = [
    {
        "pattern_id": "portfolio_overview",
        "inputs": {"portfolio_id": "test-portfolio-001", "lookback_days": 252},
        "expected_outputs": ["perf_metrics", "valued_positions", "currency_attr"],
        "field_checks": [
            ("valued_positions.positions[0].quantity", "exists"),
            ("valued_positions.positions[0].market_value", "exists"),
            ("valued_positions.positions[0].qty", "not_exists"),  # âŒ Should not exist
        ]
    },
    {
        "pattern_id": "holding_deep_dive",
        "inputs": {"portfolio_id": "test-portfolio-001", "symbol": "AAPL"},
        "expected_outputs": ["position", "fundamentals"],
        "field_checks": [
            ("position.quantity", "exists"),
            ("position.market_value", "exists"),
        ]
    },
    # ... add all 13 patterns
]

@pytest.mark.asyncio
async def test_all_patterns():
    """Test all patterns with validation."""

    db = await get_db_connection()
    runtime = AgentRuntime(db)
    orchestrator = PatternOrchestrator(runtime, db)

    results = []

    for test_case in PATTERN_TESTS:
        pattern_id = test_case["pattern_id"]
        inputs = test_case["inputs"]
        expected_outputs = test_case["expected_outputs"]
        field_checks = test_case["field_checks"]

        print(f"\nTesting {pattern_id}...")

        # Execute pattern
        ctx = RequestCtx(pricing_pack_id="PP_latest")
        result = await orchestrator.run_pattern(pattern_id, ctx, inputs)

        # Check expected outputs
        for output in expected_outputs:
            assert output in result, f"Missing output: {output}"

        # Check field names
        for field_path, check_type in field_checks:
            value = get_nested_value(result, field_path)

            if check_type == "exists":
                assert value is not None, f"Field {field_path} should exist"
            elif check_type == "not_exists":
                assert value is None, f"Field {field_path} should NOT exist"

        results.append({
            "pattern_id": pattern_id,
            "status": "PASS",
            "steps_completed": len(result.get("_trace", {}).get("steps", []))
        })

        print(f"  âœ… {pattern_id} PASS")

    # Summary
    print(f"\n{'='*60}")
    print(f"PATTERN VALIDATION SUMMARY")
    print(f"{'='*60}")
    print(f"Total patterns tested: {len(results)}")
    print(f"Passed: {sum(1 for r in results if r['status'] == 'PASS')}")
    print(f"Failed: {sum(1 for r in results if r['status'] == 'FAIL')}")

def get_nested_value(obj, path):
    """Get nested value from object using dot notation."""
    parts = path.split('.')
    value = obj

    for part in parts:
        if '[' in part:
            # Handle array index
            key, idx = part.split('[')
            idx = int(idx.rstrip(']'))
            value = value.get(key, [])[idx] if value else None
        else:
            value = value.get(part) if value else None

        if value is None:
            return None

    return value

if __name__ == '__main__':
    asyncio.run(test_all_patterns())
```

**Run Tests:**
```bash
python3 tests/integration/test_all_patterns.py
```

---

### Day 18: Production Deployment

**Task 1: Pre-Deployment Checklist**

```bash
# Checklist
- [ ] All tests pass (pattern validation, API tests, integration tests)
- [ ] Database backup created
- [ ] Rollback procedure documented
- [ ] Monitoring dashboards ready
- [ ] Alert thresholds configured
- [ ] Team notified of deployment
- [ ] Maintenance window scheduled
```

**Task 2: Staged Deployment**

**Stage 1: Database Migration (Morning)**
```bash
# Production freeze
# Enable read-only mode

# Run migrations
psql -d dawsos_production < backend/db/migrations/014_standardize_field_names.sql
psql -d dawsos_production < backend/db/migrations/015_standardize_date_fields.sql
psql -d dawsos_production < backend/db/migrations/016_add_missing_fk_constraints.sql
psql -d dawsos_production < backend/db/migrations/017_remove_duplicate_tables.sql

# Verify migrations
psql -d dawsos_production -c "SELECT COUNT(*) FROM lots;"
# Should match pre-migration count
```

**Stage 2: Code Deployment (Afternoon)**
```bash
# Deploy backend code
git pull origin main
pip install -r backend/requirements.txt
supervisorctl restart dawsos-backend

# Deploy frontend code
# (full_ui.html is already updated)

# Test basic endpoints
curl http://localhost:8000/api/health
curl http://localhost:8000/api/portfolios
```

**Stage 3: Enable Writes (Evening)**
```bash
# Disable read-only mode
# Test transaction creation
# Test pattern execution

# Monitor error rates
# Should be <1%
```

---

### Day 19-20: Production Monitoring & Optimization

**Task 1: Monitor Key Metrics**

Dashboard metrics to watch:
- API error rate (should be <1%)
- Pattern failure rate (should be <1%)
- Database query time (should be <200ms p95)
- Holdings query time (should be <50ms with materialized view)

**Task 2: Performance Optimization**

```sql
-- Analyze slow queries
SELECT
  query,
  mean_exec_time,
  calls
FROM pg_stat_statements
WHERE mean_exec_time > 100  -- >100ms
ORDER BY mean_exec_time DESC
LIMIT 20;

-- Add indexes if needed
CREATE INDEX idx_custom ON table_name(column) WHERE condition;
```

---

### Week 4 Completion Checklist

**Testing:**
- [ ] All 13 patterns tested
- [ ] API endpoint tests pass
- [ ] Integration tests pass
- [ ] Performance tests pass

**Production:**
- [ ] Database migrations deployed
- [ ] Code deployed
- [ ] Monitoring active
- [ ] Error rates normal (<1%)

**Documentation:**
- [ ] Migration guide updated
- [ ] API documentation updated
- [ ] Runbook updated

---

## ðŸ“‹ Success Metrics

### Technical Metrics (Target)

| Metric | Before | After | Target | Status |
|--------|--------|-------|--------|--------|
| Field name consistency | 30% | 95%+ | >90% | â³ |
| Holdings query time | 150ms | <50ms | <100ms | â³ |
| Pattern success rate | 95% | 99%+ | >98% | â³ |
| API error rate | 3% | <1% | <2% | â³ |
| FK constraint coverage | 60% | 100% | 100% | â³ |

### User Experience Metrics

| Metric | Before | After | Target | Status |
|--------|--------|-------|--------|--------|
| Portfolio load time | 2s | <1s | <1.5s | â³ |
| Pattern execution time | 3s | <2s | <2.5s | â³ |
| Token refresh success | N/A | 99%+ | >95% | â³ |
| Error message clarity | 30% | 80%+ | >70% | â³ |

---

## ðŸš¨ Risk Mitigation

### Rollback Triggers

**Automatic Rollback If:**
- Error rate >5% for >5 minutes
- Pattern failure rate >10%
- Database connection errors

**Manual Rollback If:**
- Data integrity issues discovered
- Critical bug found in production
- User reports of data loss

### Rollback Procedure

```bash
#!/bin/bash
# rollback.sh

echo "=== EMERGENCY ROLLBACK ==="

# Stop application
supervisorctl stop dawsos-backend

# Restore database
pg_restore -d dawsos_production --clean backup_20251104_production.dump

# Revert code
git checkout production-stable-20251103
pip install -r backend/requirements.txt

# Restart application
supervisorctl start dawsos-backend

echo "=== ROLLBACK COMPLETE ==="
```

---

## ðŸ“ Final Notes

### What Makes This Plan Different

1. **Field Names First**: Recognizes that pattern system depends on standardized field names
2. **Database Foundation**: Fixes database issues before code refactoring
3. **Validation at Every Layer**: Pydantic schemas + UI validation + database constraints
4. **Staged Deployment**: Read-only â†’ Code â†’ Writes to minimize risk
5. **Clear Dependencies**: Each week builds on previous week's completion

### Total Effort Estimate

- **Week 0 (Foundation)**: 3-5 days
- **Week 1-2 (Pattern System)**: 10 days
- **Week 3 (Complete Fixes)**: 5 days
- **Week 4 (Validation)**: 5 days
- **Total: 23-27 days (4-5 weeks)**

### Team Requirements

- Database Engineer (Week 0, Week 4)
- Backend Engineers (Week 0-3)
- Frontend Engineer (Week 1-2, Week 4)
- QA Engineer (Week 4)
- DevOps (Week 0, Week 4)

---

**Plan Complete**
**Status:** ðŸ“‹ Ready for Execution
**Next Step:** Review with stakeholders, get approval for Week 0 start

---

**Document Generated:** November 4, 2025
**Generated By:** Claude (Anthropic)
**Version:** 1.0
